<!DOCTYPE html>

<html>
  
  <head>
    <head>
    <img src="download.png" alt="Banner" wdith="1800" height="300" >
       <h1><b>ENG0018 Computer Laboratory 2023/24</b></h1
       <h2>Student URN: Project</h2>
 <hr>
 <h2 style:fomt-family:calibri;"><b>Conference paper: chosen topic</b></h2>
 <hr>
      <!style for tables>
      <style>
        table {
          font-family: arial, sans-serif;
          boarder-collapse: collapse;
          width 30%

        }

        td, th {
          border: 1px solid #dddddd;
          text-align: left;
          padding: 8px;
        }

        tr:nth-child(even) {
          background-colour: #dddddd;
        }



        
      </style>
    
    <style>
      p.ex1 {
      margin-left: 250px;
      }
    </style>
    </head> </head>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<style>
  .mySlides {display:none;}
</style>


  <!table of contents>
  <table>

    <tr>
      <th><h3>Table of contents</h3></th>
    </tr>
    <tr>
      <td><a href="#Abstract">Abstract</td>
    </tr>
    <tr>
      <td><a href="#Introduction">Introduction</td>
    </tr>
    <tr>
      <td><a href="#Analysis and discussion">Analysis and discussion</td>
    </tr>
  </table>
  

  <body style="background-color:#FFFFFF;margin-left:50px;">
  </body>
    <hr>
    <h3 id="Abstract">Abstract</h3>
    <p class:"ex1">
      </p>
      <pre>
        New AI generative programs now becoming accessible to consumers, AI generated art has invited intrigue and discussion on how we now define art. This
        article is a thought exercise on making metaphysical and analytical comparisons between how AI models are generating art and the neuroscientific
        perspective on how humans conceptualise and create art. Beginning with a simple overview of the four neural networks the stable diffusion model uses,
        and the regions of the brain involved with motor learning and processing semantic information.
      </pre>

  </body>
    <hr>
    <h3 id="Introduction">Introduction</h3>
    <p class:"ex1">
      </p>
      <pre>
        An article in the 1999 QPB Science Encyclopaedia, written by Peter Lafferty, discusses the topic of artificial neural networks being developed as a
        new model of computation deviating from programming. ‘They are called neural networks because they are loosely modelled on the networks of neurons
        that make up brains. Neural networks are characterized by their ability to learn and can be described as trainable pattern recognisers.’ (p.518). 
        Computation modelled to reproduce the learning capabilities of organic neural networks continuously developed onwards. This research and development
        progressed into AI generative programs that are now opened to commercial public access, ChatGPT can be pointed to as a popular example. However, this
        article is specifically going to focus on AI ‘illustrative art’ generative programs such as Midjourney, DALL-E, Stable Diffusion etc.
      </pre>

    </body>
    <hr>
    <h3 id="Analysis and discussion">Analysis and discussion</h3>
    <p class:"ex1">
      </p>
      <pre>
        These programs use a deep learning model called stable diffusion to generate images. Stable Diffusion is ‘trained’ on many images in certain data sets
        through a process involving encoders and decoders referred to as variational autoencoders. It takes an input image represented by pixels and encodes it
        into a latent space. The ‘convolutional layer’ allows for efficient semantic segmentation which targets localization and the perimeters of the subject in
        the image which it is being trained on. (Ronneberger, Fischer and Brox, 2015, p. 4). This is useful as it allows for decompression of input data and then
        increasing its resolution again in the output while the abstract and subjective interpretation of an image is still processed by the model. More efficient 
        than if each pixel in the image input was encoded to a vector and required decoding back to its original format. Stable diffusion iteratively adds gaussian
        noise after image is encoded into a latent space until the image is ‘overwritten’ with random noise. This random noise is generated from the gaussian 
        probability distribution and is ‘isotropic’. (Alammar, 2022, Rocca, 2022) The stable diffusion model is trained by restoring its input image before gaussian
        noise is added, this generates an output that is assessed in comparison to its input, wanting to reverse the process from a simple normal distribution noise
        to a more complex one like the original image. Enough images the model is trained in, and enough generations made assessed against the ‘ground truth’ image,
        the model’s algorithm adjusts to generate ‘meaningful’ images. (Alammar, 2022, Rocca, 2022). 

<img src="noise nosie.jpg" alt="Banner" wdith="100" height="16" >
        
        These ‘meaningful’ images can also be classed into its language equivalent, using a mechanism called the cross-attention mechanism. Using a different embedding
        model ‘Word2Vec’ (‘Latent Space’, 2023), just like the variational autoencoders embed an images’ pixels into continuous vectors, Word2Vec also embeds words to
        vectors. The latent space in the Word2Vec model mimics the pattern recognition capabilities emergent of organic neural networks by embedding words into vectors,
        if these vectors have a greater dot product the more similar or relational these words are.(‘Word2vec’, 2023)A greater dot product of two vectors means that the
        vectors are closer in value and in this latent space are mathematically ‘closer together’.  
        
        What defines learning in the human brain however is neuroplasticity and is defined as ‘changes in the brain that are caused by activity or experience.’
        (Cherry, 2022). The radioisotope carbon 14 had increased concentration in the atmosphere after nuclear bomb testing's (Fuchs, Flugge, 2014), researchers 
        were able to use carbon dating to learn the date of formation of cells in the hippocampus. This was possible because ‘when a cell goes through mitosis and duplicates
        its chromosomes, it integrates carbon 14 in the synthesized genomic DNA with a concentration corresponding to that in the atmosphere at the time, creating a date mark
        in the DNA.’ (Spalding et al, 2013). This analysis also revealed adult neurogenesis in the striatum, ‘The synaptic pathways in the striatum are central to basal ganglia
        functions including motor control, learning and organization, action selection, acquisition of motor skills’, damage to striatum results in major dysfunction in
        voluntary movement, which is observed in Parkinson's disease and Huntington's disease (Cataldi et al, 2021). The striatum is also associated with the stimuli-response
        model, where reinforcement through the stimuli increases predictability of the response, typically an involuntary movement. In multi-voxel pattern classification studies,
        mental imagery is described as a reactivation of the visual information stored from and during perception only at a ‘weaker threshold’. ‘This finding implies that low-level
        visual features (e.g., space, spatial frequency, and orientation) are encoded during mental imagery.’(Naselaris et al, 2015, pp. 215-228). Visual information is also stored 
        in relation to one another in a ‘neural ensemble’, these connections are described as being ’strengthened‘ when the synaptic potential (potential difference) is measured 
        at the neurons synapse. This process of strengthening is also attributed to development of fine motor skills. This overview of how neural and motor processes allow for the 
        capability for humans to create art. 

        The creative capabilities of human-made art and ai generated art are very closely comparable, it is a derivative process where the abstract concept of originality is 
        difficult to define in this context. There is an input with semantic meaning and semantic classifications can be used to combine concepts. AI programs like Midjourney
        used images ‘scraped’ from across the internet, websites that platform artists such as ArtStation and DeviantArt that already come with an image to word classification 
        due its tagging and captioning system. Suggesting it was likely these models were trained on artworks uploaded on these websites (Maker 2023). Since neuroscientific research
        into illustrative creativity is sparse, we must rely on a philosophical framework instead. Semantic classification is intuitive and inherent to human thought and this
        understanding of our perceptions is processed as relational; it emerges the ability of recognising patterns in our environment but also ‘see’ relationships where it is not
        reflected in ‘reality’ as rigidly. ‘The creator has to transform the conceptual space itself, by altering its constitutive rules or constraints.’, this is Boden‘s
        ’transformational creativity’ (Stanford Encyclopedia of Philosophy, 2023). AI art relies on existing images to be classed; it still requires mathematical understanding of
        what it is compositing. Humans however create abstraction in art like in cartoon animation or impressionist style paintings without the pre-existing concept needing to be learned.
      </pre>
  
<p id="date&time">
    </p>
  <script>
    
    //Javascript to display the time and date on my webpage
    
    const d = new Date();
    document.getElementById("date&time").innerHTML = d;
    
  </script>
  
  </body>
    <hr>
    <h3 id="References">References</h3>
    <p class:"ex1">
      </p>
      <pre>
        Lafferty, P. (1999) <em>QPB science encyclopedia</em>. New York: Quality Paperback Book Club
        Ronneberger, O., Fischer, P. and Bronx, T. (2015) <em>U-Net : Convolutional Networks for Biomedical Image Segmentation</em>. Available at:https://arxiv.org/pdf/1505.04597.pdf (Accessed: 5th December 2023)
        Alammar, J. (2022) <em>The Illustrated Stable Diffusion</em>. Available at: https://jalammar.github.io/illustrated-stable-diffusion/ (Accessed: 5th December 2023)
        Rocca, J. (2022) <em>Understanding Diffusion Probabilistic Modes</em>. Available at: https://towardsdatascience.com/understanding-diffusion-probabilistic-models-dpms-1940329d6048 (Accessed: 6th December 2023)
        'Latent Space' (2023) <em>Wikipedia</em>. Available at: https://en.wikipedia.org/wiki/Latent_space (Accessed: 6th December 2023)
        'Word2Vec' (2023) <em>Wikipedia</em>. Available at: https://en.wikipedia.org/wiki/Word2vec (Accessed: 6th December 2023)
        Cherry, K. (2022) <em>What is Neuroplasticity</em>. Available at: https://www.verywellmind.com/what-is-brain-plasticity-2794886 (Accessed: 6th December 2023)
        Fuchs, E. and Flugge, G. (2014) <em>Adult Neuroplasticity: More Than 40 Years of Research</em>. Available at: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4026979/ (Accessed: 6th December 2023)
        Spalding, K.L. et al (2013) <em>Dynamics of hippocampal neurogenesis in adult humans</em>. Available at: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4394608/ (Accessed: 6th December 2023)
        Cataldi, S., Stanley, A.T., Miniaci, M.C. and Sulzer, D. (2021) <em>Interpreting the role of the striatum during multiple phases of motor learning</em>. Available at: https://pubmed.ncbi.nlm.nih.gov/33977645/ (Accessed: 6th December 2023)
        Naselaris, T. et al, (2015) <em>A voxel-wise encoding model for early visual areas decodes mental images of remembered scenes</em>. Available at: https://www.sciencedirect.com/science/article/pii/S1053811914008428 (Accessed: 6th December 2023)
        Maker, A. (2023) <em>How Midjourney was Trained</em>. Available at: https://imaigic.com/blog/how-mj-was-trained#:~:text=The%20dataset%20used%20in%20training,in%20Context%20(COCO)%20dataset. (Accessed: 6th December 2023)
        Stanford Encyclopedia of Philosophy (2023) <em>Creativity</em>. Available at: https://plato.stanford.edu/entries/creativity/#:~:text=Boden%20calls%20this%20transformational%20creativity,its%20constitutive%20rules%20or%20constraints. (Accessed: 7th December 2023)
      </pre>


      
    
    
